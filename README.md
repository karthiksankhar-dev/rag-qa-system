# RAG Question Answering System

A powerful Retrieval Augmented Generation (RAG) application that combines real-time web search, Wikipedia knowledge, and local language models to provide comprehensive answers to user questions.

## üöÄ Features

- **Multi-Source Knowledge Retrieval**: Integrates Google Search and Wikipedia for comprehensive information gathering
- **Local LLM Processing**: Uses Ollama with Llama 3.1 for intelligent response generation
- **Vector Database Storage**: Implements Chroma DB with NVIDIA embeddings for semantic search
- **Real-time Streaming**: Provides live response generation with custom UI feedback
- **Persistent Knowledge Base**: Maintains conversation history and learned information
- **User-Friendly Interface**: Clean Streamlit web application with expandable output containers

## üõ†Ô∏è Prerequisites

Before running this application, ensure you have:

- Python 3.8 or higher
- [Ollama](https://ollama.ai/) installed and running
- Google Serper API key
- NVIDIA API key
- At least 8GB RAM (recommended for local LLM)

## üì¶ Installation

### 1. Clone the Repository

