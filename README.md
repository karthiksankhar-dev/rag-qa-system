
# RAG Question Answering System

A powerful Retrieval Augmented Generation (RAG) application that combines real-time web search, Wikipedia knowledge, and local language models to provide comprehensive answers to user questions.
ğŸš€ Features
Multi-Source Knowledge Retrieval: Integrates Google Search and Wikipedia for comprehensive information gathering
Local LLM Processing: Uses Ollama with Llama 3.1 for intelligent response generation
Vector Database Storage: Implements Chroma DB with NVIDIA embeddings for semantic search
Real-time Streaming: Provides live response generation with custom UI feedback
Persistent Knowledge Base: Maintains conversation history and learned information
User-Friendly Interface: Clean Streamlit web application with expandable output containers
ğŸ› ï¸ Prerequisites
Before running this application, ensure you have:
Python 3.8 or higher
[Ollama](https://ollama.ai/)Â installed and running
Google Serper API key
NVIDIA API key
At least 8GB RAM (recommended for local LLM)
ğŸ“¦ Installation

1. Clone the Repository
bash
git clone [https://github.com/yourusername/rag-qa-system.git](https://github.com/yourusername/rag-qa-system.git)
cd rag-qa-system
2. Create Virtual Environment
bash
python -m venv venv
source venv/bin/activate  \# On Windows: venv\Scripts\activate
3. Install Dependencies
bash
pip install streamlit
pip install langchain-community
pip install langchain-ollama
pip install langchain-nvidia-ai-endpoints
pip install chromadb
pip install google-search-results
pip install wikipedia
Or install from requirements file:
bash
pip install -r requirements.txt
4. Install and Setup Ollama
bash

# Install Ollama (visit [https://ollama.ai/](https://ollama.ai/) for OS-specific instructions)

# Pull the required model

ollama pull llama3.1
5. Setup API Keys
Create a file namedÂ id.pyÂ in the project root directory:
python

# id.py

SERPER_API_KEY = "your_serper_api_key_here"
nvapi_key = "your_nvidia_api_key_here"
ğŸ”‘ API Key Setup
Google Serper API
VisitÂ [Serper.dev](https://serper.dev/)
Sign up for a free account
Get your API key from the dashboard
Add it to yourÂ id.pyÂ file
NVIDIA API
VisitÂ [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/)
Sign up and get your API key
Add it to yourÂ id.pyÂ file
ğŸš€ Usage

1. Start the Application
bash
streamlit run app.py
2. Access the Interface
Open your browser and navigate toÂ http://localhost:8501
3. Ask Questions
Enter your question in the text input field
Watch as the system retrieves information and generates answers in real-time
Expand the output container to see the complete response
ğŸ“ Project Structure
text
rag-qa-system/
â”œâ”€â”€ app.py                 \# Main application file
â”œâ”€â”€ id.py                  \# API keys configuration
â”œâ”€â”€ requirements.txt       \# Python dependencies
â”œâ”€â”€ README.md             \# Project documentation
â”œâ”€â”€ chroma_db/            \# Vector database storage (created automatically)
â””â”€â”€ .gitignore           \# Git ignore file
ğŸ”§ Configuration
Customizing the LLM
To use a different Ollama model, modify theÂ llmÂ initialization inÂ app.py:
python
llm = ChatOllama(model="your-preferred-model", temperature=0, streaming=True)
Adjusting Text Chunking
Modify the text splitter parameters for different chunk sizes:
python
splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
Vector Database Location
Change the persistence directory for the Chroma database:
python
persist_directory = "./your-custom-db-path"
ğŸ“‹ Requirements.txt
text
streamlit>=1.28.0
langchain-community>=0.0.20
langchain-ollama>=0.1.0
langchain-nvidia-ai-endpoints>=0.1.0
chromadb>=0.4.15
google-search-results>=2.4.2
wikipedia>=1.4.0
langchain>=0.1.0
ğŸ› Troubleshooting
Common Issues
4. Ollama Connection Error
bash

# Ensure Ollama is running

ollama serve

# Pull the required model

ollama pull llama3.1
2. API Key Errors
Verify your API keys are correctly set inÂ id.py
Check API key quotas and permissions
3. ChromaDB Persistence Issues
Delete theÂ chroma_dbÂ folder and restart the application
Ensure proper write permissions in the project directory
4. Memory Issues
Reduce chunk size in the text splitter
Close other memory-intensive applications
ğŸ”’ Security Notes
Never commit yourÂ id.pyÂ file to version control
AddÂ id.pyÂ to yourÂ .gitignoreÂ file
Use environment variables in production deployments
ğŸ¤ Contributing
Fork the repository
Create a feature branch (git checkout -b feature/amazing-feature)
Commit your changes (git commit -m 'Add some amazing feature')
Push to the branch (git push origin feature/amazing-feature)
Open a Pull Request
ğŸ“„ License
This project is licensed under the MIT License - see theÂ [LICENSE](https://www.perplexity.ai/search/LICENSE)Â file for details.
ğŸ™ Acknowledgments
[LangChain](https://python.langchain.com/)Â for the RAG framework
[Ollama](https://ollama.ai/)Â for local LLM hosting
[Streamlit](https://streamlit.io/)Â for the web interface
[ChromaDB](https://www.trychroma.com/)Â for vector storage
[NVIDIA](https://www.nvidia.com/)Â for embedding models
ğŸ“ Support
If you encounter any issues or have questions:
Open an issue on GitHub
Check the troubleshooting section above
Review the official documentation for each technology used
I need the above to be in a single .md code file

Got it âœ… â€” you want the entire project documentation written as a single Markdown (`.md`) file.
Hereâ€™s the properly formatted `README.md` content you can save directly as a Markdown file:

```markdown
# ğŸš€ RAG Question Answering System

A powerful **Retrieval Augmented Generation (RAG)** application that combines real-time web search, Wikipedia knowledge, and local language models to provide **comprehensive, intelligent answers** to user questions.

---

## âœ¨ Features
- **Multi-Source Knowledge Retrieval**: Integrates Google Search and Wikipedia for comprehensive information gathering  
- **Local LLM Processing**: Uses [Ollama](https://ollama.ai/) with *Llama 3.1* for intelligent response generation  
- **Vector Database Storage**: Implements **Chroma DB** with NVIDIA embeddings for semantic search  
- **Real-time Streaming**: Provides live response generation with custom UI feedback  
- **Persistent Knowledge Base**: Maintains conversation history and learned information  
- **User-Friendly Interface**: Clean **Streamlit** web application with expandable output containers  

---

## ğŸ› ï¸ Prerequisites
Before running this application, ensure you have:
- **Python 3.8+**
- **Ollama** installed and running
- **Google Serper API key**
- **NVIDIA API key**
- At least **8GB RAM** (recommended for local LLM)

---

## ğŸ“¦ Installation

### 1. Clone the Repository
```

git clone https://github.com/yourusername/rag-qa-system.git
cd rag-qa-system

```

### 2. Create Virtual Environment
```

python -m venv venv
source venv/bin/activate   \# On Windows: venv\Scripts\activate

```

### 3. Install Dependencies
```

pip install streamlit
pip install langchain-community
pip install langchain-ollama
pip install langchain-nvidia-ai-endpoints
pip install chromadb
pip install google-search-results
pip install wikipedia

```

Or install everything from requirements:
```

pip install -r requirements.txt

```

### 4. Install and Setup Ollama
```


# Install Ollama (see https://ollama.ai/ for OS-specific instructions)

# Pull the required model

ollama pull llama3.1

```

### 5. Setup API Keys
Create a file named **id.py** in the project root:
```


# id.py

SERPER_API_KEY = "your_serper_api_key_here"
nvapi_key = "your_nvidia_api_key_here"

```

---

## ğŸ”‘ API Key Setup

### Google Serper API
1. Visit [Serper.dev](https://serper.dev/)  
2. Sign up for a free account  
3. Get your API key from the dashboard  
4. Add it to `id.py`  

### NVIDIA API
1. Visit [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/)  
2. Sign up and get your API key  
3. Add it to `id.py`  

---

## ğŸš€ Usage

### 1. Start the Application
```

streamlit run app.py

```

### 2. Access the Interface
Open your browser: **http://localhost:8501**

### 3. Ask Questions
- Enter your question in the input field  
- Watch as the system retrieves and generates answers in real-time  
- Expand the output to view the detailed response  

---

## ğŸ“ Project Structure
```

rag-qa-system/
â”œâ”€â”€ app.py                 \# Main application file
â”œâ”€â”€ id.py                  \# API keys configuration
â”œâ”€â”€ requirements.txt       \# Python dependencies
â”œâ”€â”€ README.md              \# Project documentation
â”œâ”€â”€ chroma_db/             \# Vector database storage (auto-generated)
â””â”€â”€ .gitignore             \# Git ignore file

```

---

## ğŸ”§ Configuration

### Customizing the LLM
```

llm = ChatOllama(model="your-preferred-model", temperature=0, streaming=True)

```

### Adjusting Text Chunking
```

splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=300)

```

### Vector Database Location
```

persist_directory = "./your-custom-db-path"

```

---

## ğŸ“‹ requirements.txt
```

streamlit>=1.28.0
langchain-community>=0.0.20
langchain-ollama>=0.1.0
langchain-nvidia-ai-endpoints>=0.1.0
chromadb>=0.4.15
google-search-results>=2.4.2
wikipedia>=1.4.0
langchain>=0.1.0

```

---

## ğŸ› Troubleshooting

### 1. Ollama Connection Error
```


# Ensure Ollama is running

ollama serve

# Pull required model

ollama pull llama3.1

```

### 2. API Key Errors
- Verify keys are in `id.py`  
- Check API key quotas and permissions  

### 3. ChromaDB Issues
- Delete the `chroma_db` folder and restart  
- Ensure the project directory has write permissions  

### 4. Memory Issues
- Reduce chunk size in the text splitter  
- Close other memory-heavy programs  

---

## ğŸ”’ Security Notes
- âŒ Never commit your `id.py` file to GitHub  
- Add `id.py` to `.gitignore`  
- ğŸ”‘ Use environment variables for deployment  

---

## ğŸ¤ Contributing
1. Fork the repo  
2. Create a feature branch:  
```

git checkout -b feature/amazing-feature

```
3. Commit changes:  
```

git commit -m "Add some amazing feature"

```
4. Push branch:  
```

git push origin feature/amazing-feature

```
5. Open a Pull Request  

---

## ğŸ“„ License
This project is licensed under the **MIT License** â€“ see the LICENSE file for details.

---

## ğŸ™ Acknowledgments
- [LangChain](https://python.langchain.com/) â€“ for the RAG framework  
- [Ollama](https://ollama.ai/) â€“ for local LLM hosting  
- [Streamlit](https://streamlit.io/) â€“ for UI  
- [ChromaDB](https://www.trychroma.com/) â€“ for vector storage  
- [NVIDIA](https://www.nvidia.com/) â€“ for embeddings  

---

## ğŸ“ Support
- Open an **issue on GitHub**  
- Check the **Troubleshooting** section above  
- Review the official **documentation** of each dependency  

```


***

Would you like me to also create a **minimal ready-to-run `requirements.txt` file** separately along with this `README.md`? That way you can just copy both into your repo and start immediately.

